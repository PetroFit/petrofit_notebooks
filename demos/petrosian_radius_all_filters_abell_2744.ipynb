{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from astropy.nddata import CCDData, Cutout2D\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.stats import sigma_clipped_stats, sigma_clip\n",
    "from astropy import units as u\n",
    "\n",
    "from photutils import source_properties, SourceProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from petrofit.segmentation import make_segments, deblend_segments, plot_segments\n",
    "from petrofit.petrosian import Petrosian, PetrosianCorrection\n",
    "from petrofit.photometry import get_source_position, object_photometry, flux_to_abmag\n",
    "from petrofit.utils import angular_to_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 12]\n",
    "plt.rcParams['image.origin'] = 'lower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "DISPLAY_STYLE = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-bloom",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-tenant",
   "metadata": {},
   "source": [
    "In this section we define the paths to the Hubble Space Telescope Frontier Fields. The fields are located on the [STScI data archive website]( https://archive.stsci.edu/pub/hlsp/frontier). There are 6 clusters and each cluster has 7 band images (3 ACS and 4 WFC3). Here we generate and store the URL paths to different band images of a single cluster so we can download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_filter_list = ['f435w', 'f606w', 'f814w']\n",
    "wfc3_filter_list = ['f105w', 'f125w', 'f140w', 'f160w']\n",
    "\n",
    "input_data_formatter = \"https://archive.stsci.edu/pub/hlsp/frontier/abell2744/images/hst/v1.0/hlsp_frontier_hst_{}-60mas_abell2744_{}_v1.0_drz.fits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_name = os.path.basename(input_data_formatter).replace(\"_{}\",\"\").replace('-','')\n",
    "program_name = os.path.splitext(program_name)[0]\n",
    "program_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {}\n",
    "for f in acs_filter_list:\n",
    "    data_paths[f] = input_data_formatter.format('acs', f)\n",
    "    \n",
    "for f in wfc3_filter_list:\n",
    "    data_paths[f] = input_data_formatter.format('wfc3', f)\n",
    "    \n",
    "petrosian_cat_collection = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in acs_filter_list + wfc3_filter_list:\n",
    "    print(\"{}: {}\".format(f, data_paths[f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-grove",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-fruit",
   "metadata": {},
   "source": [
    "We select the center of the cluster in this section and the size of the cutout that will be analyzed. We also specify the center and cutout size of a patch of the image that we will use to estimate the background noise stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots = True\n",
    "\n",
    "# Dark patch:\n",
    "# A dark patch in the image to measure background stats\n",
    "noise_cutout_center = (2760, 3420)\n",
    "noise_cutout_size = 70\n",
    "\n",
    "# Center of cluster:\n",
    "cx, cy = [2412, 2519]\n",
    "data_cutout_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-remove",
   "metadata": {},
   "source": [
    "# Detection Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-symbol",
   "metadata": {},
   "source": [
    "Before we can analyze images of galaxies, we first need to make a catalog of their positions. To locate galaxies, we first select a band of interest and download the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_filter = 'f105w'\n",
    "input_data_path = data_paths[detection_filter]\n",
    "detection_data = CCDData.read(input_data_path, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-click",
   "metadata": {},
   "source": [
    "### Estimate detection data noise at dark area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-enclosure",
   "metadata": {},
   "source": [
    "The following cell makes a cutout of the noise image (cutout of dark patch we specified) and measure the mean, std and sigma values for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_noise_cutout = Cutout2D(detection_data, noise_cutout_center, noise_cutout_size)\n",
    "\n",
    "detection_noise_mean = detection_noise_cutout.data.mean()\n",
    "detection_noise_sigma = detection_noise_cutout.data.std()\n",
    "detection_noise_3_sigma = detection_noise_sigma * 3.\n",
    "detection_noise_8_sigma = detection_noise_sigma * 8.\n",
    "\n",
    "print(detection_noise_mean, detection_noise_3_sigma, detection_noise_8_sigma)\n",
    "    \n",
    "if show_plots:\n",
    "    plt.imshow(detection_noise_cutout.data, \n",
    "               vmax=detection_noise_mean+detection_noise_3_sigma, \n",
    "               vmin=detection_noise_mean-detection_noise_3_sigma)\n",
    "    plt.xlabel('Pix')\n",
    "    plt.ylabel('pix')\n",
    "    plt.show()\n",
    "    \n",
    "    n, bins, patches = plt.hist(detection_noise_cutout.data.flatten(), bins=35, align='left', color='black')\n",
    "    plt.plot(bins[:-1], n, c='r', linewidth=3)\n",
    "\n",
    "    plt.xlabel('Flux Bins [{}]'.format(str(detection_data.unit)))\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Noise Histogram')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-orleans",
   "metadata": {},
   "source": [
    "### Cutout Detection Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-lease",
   "metadata": {},
   "source": [
    "In this section we make a cutout of the cluster at the user specified center and size. We also compute some stats that will become useful in the detection stage. We also use the detection image std to set the `vmax` and `vmin` of the detection image plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_image = Cutout2D(detection_data, (cx, cy), data_cutout_size, copy=True)\n",
    "\n",
    "# Compute image stats\n",
    "detection_image_min = detection_image.data.min()\n",
    "detection_image_max = detection_image.data.max()\n",
    "detection_image_mean = detection_image.data.mean()\n",
    "detection_image_sigma = detection_image.data.std()\n",
    "detection_image_3_sigma = detection_image_sigma * 3.\n",
    "detection_image_8_sigma = detection_image_sigma * 8.\n",
    "\n",
    "detection_image_clipped_mean, detection_image_clipped_median, detection_image_clipped_std = sigma_clipped_stats(detection_image.data, sigma=3.0)\n",
    "\n",
    "# Subtract Noise\n",
    "# detection_image.data  -= detection_image_clipped_mean\n",
    "# detection_image.data = np.clip(detection_image.data, -detection_image_3_sigma, np.inf)\n",
    "\n",
    "# Set min and max values for all plots\n",
    "vmin = -10 * detection_image_clipped_std\n",
    "vmax = +10 * detection_image_clipped_std\n",
    "\n",
    "if show_plots:\n",
    "    plt.imshow(detection_image.data, vmin=vmin, vmax=vmax)\n",
    "    plt.xlabel('Pix')\n",
    "    plt.ylabel('pix')\n",
    "    plt.show()\n",
    "\n",
    "#print(image_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-sussex",
   "metadata": {},
   "source": [
    "### Segmentation and Main Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-price",
   "metadata": {},
   "source": [
    "In this section we use PhotUtils to make a segmentation map and deblend the identified sources. Please note that `make_segments` and `deblend_segments` are wrappers for their respective PhotUtils functions that make it easy to pass arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detect threshold\n",
    "nsigma = np.ones_like(detection_image.data)\n",
    "nsigma[:, :] = detection_noise_3_sigma\n",
    "\n",
    "# Define smoothing kernel\n",
    "kernel_size = 5\n",
    "fwhm = 5\n",
    "npixels = 5**2\n",
    "\n",
    "# Make segmentation map\n",
    "detection_segm = make_segments(detection_image.data, nsigma=nsigma, kernel_size=kernel_size, fwhm=fwhm, npixels=npixels)\n",
    "\n",
    "# Deblend segmentation map\n",
    "detection_segm_deblend = deblend_segments(detection_image.data, detection_segm, contrast=0, nlevels=50,\n",
    "                                         kernel_size=kernel_size, fwhm=fwhm, npixels=npixels)\n",
    "\n",
    "# Remove border sources\n",
    "#detection_segm_deblend.remove_border_labels(200)\n",
    "\n",
    "# Make catalog\n",
    "detection_cat = source_properties(detection_image.data, detection_segm_deblend, wcs=detection_image.wcs)\n",
    "\n",
    "# Display source properties\n",
    "print(\"Num of Targets:\", len(detection_cat))\n",
    "\n",
    "# Plot segments\n",
    "if show_plots:\n",
    "    plot_segments(detection_segm_deblend, detection_image.data, vmin=vmin, vmax=vmax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-statistics",
   "metadata": {},
   "source": [
    "# Measure Petrosian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-protein",
   "metadata": {},
   "source": [
    "We measure here the Petrosian radii of sources by analyzing their radial light profiles. To do this we select the max pixel to be equivalent to 15 arcsec and sample at every pixel up to the max pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Radii\n",
    "# ------------\n",
    "# Calculate galaxy size\n",
    "galaxy_pixel_size = angular_to_pixel(15 * u.arcsec, wcs=detection_image.wcs) # convert to pixels\n",
    "\n",
    "# Define max rad in pixels and number of apertures\n",
    "max_pix = int(round(galaxy_pixel_size))\n",
    "n = int(max_pix)\n",
    "\n",
    "# Create list of radii\n",
    "r_list = [x * int(max_pix / n) for x in range(1, n+1)]\n",
    "r_list = np.array(r_list)\n",
    "\n",
    "print(\"galaxy_pixel_size = {:0.2f} pix\".format(galaxy_pixel_size))\n",
    "print(\"max_pix = {} pix\".format(max_pix, n))\n",
    "print(\"n = {}\".format(max_pix, n))\n",
    "print(\"len(r_list) = {}\".format(len(r_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-making",
   "metadata": {},
   "source": [
    "## Detection Photometry and Petrosian Calculation \n",
    "\n",
    "For each source in the detection catalog, measure the Petrosian properties. Before making the measurement, we measure the photometery of the objects by :\n",
    "1. Estimate a good size for the cutout around the target source (`cutout_size`) using the size of the segmentation box.\n",
    "2. Clip the radius array if it is larger than the cutout image (to save compute time).\n",
    "3. Mask all sources that are not the target.\n",
    "4. Make a cutout of the target.\n",
    "5. To subtract the background around the source, we fit a 2D plane to the cutout pixels that are below the `sigma` parameter. We then turn the fitted 2D plane model into an image, which we subtract from the cutout. \n",
    "6.  We measure the photometry at each radius in the clipped radius list. The shape of the apertures is elliptical. The elongation and orientation of the ellipse are determined by the of the source in the segmentation output.\n",
    "7. We then use the output of the photometry to construct a curve of growth and construct a  `Petrosian` object which can be used to infer Petrosian properties (radius, C2080 etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-sunset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pb = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(detection_cat),\n",
    "    step=1,\n",
    "    description='Loading:',\n",
    "    bar_style='',\n",
    "    orientation='horizontal',\n",
    "    style=DISPLAY_STYLE,\n",
    ")\n",
    "display(pb)\n",
    "\n",
    "petrosian_properties_rows = []\n",
    "\n",
    "for idx, obj in enumerate(detection_cat):\n",
    "    pb.value = idx + 1\n",
    "    pb.description = \"{}: {}/{}\".format(detection_filter, pb.value, len(detection_cat))\n",
    "    \n",
    "    bbox = max(obj.bbox.ixmax - obj.bbox.ixmin, obj.bbox.iymax - obj.bbox.iymin)\n",
    "    max_r = bbox * 3\n",
    "    cutout_size = bbox * 3\n",
    "    \n",
    "    flux_arr, area_arr, error_arr = object_photometry(\n",
    "        obj, detection_image.data, \n",
    "        detection_segm_deblend, r_list[:max_r], \n",
    "        plot=False, sigma=1, sigma_type='clip',\n",
    "        vmax=vmax/10, vmin=0, \n",
    "        bkg_sub=True, mask_background=False,\n",
    "        cutout_size=cutout_size, #method='center'\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    p = Petrosian(r_list[:max_r], area_arr[:max_r], flux_arr[:max_r],)\n",
    "    petrosian_properties_rows.append(p)\n",
    "    \n",
    "    if False:\n",
    "        p.plot(plot_r=False, plot_normalized_flux=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-tours",
   "metadata": {},
   "source": [
    "# Photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometry_process_image(current_filter, show_plots=False):\n",
    "    print(current_filter + \" \")\n",
    "\n",
    "    input_data_path = data_paths[current_filter]\n",
    "    output_path = \"{}_petrosian_catalog.csv\".format(os.path.splitext(os.path.basename(input_data_path))[0])\n",
    "    \n",
    "    # Load data and vitals\n",
    "    # ---------------------\n",
    "    #print('Load data and vitals')\n",
    "    data = CCDData.read(input_data_path)\n",
    "    rms_data = fits.getdata(input_data_path.replace('drz', 'rms'))\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.imshow(data, vmin=0, vmax=data.data.mean()*10)\n",
    "        plt.show()\n",
    "        \n",
    "    # Estimate data noise at dark area\n",
    "    # --------------------------------\n",
    "    #print('Estimate data noise at dark area')\n",
    "    noise_cutout = Cutout2D(data, noise_cutout_center, noise_cutout_size)\n",
    "\n",
    "    noise_mean = noise_cutout.data.mean()\n",
    "    noise_sigma = noise_cutout.data.std()\n",
    "    noise_3_sigma = noise_sigma * 3.\n",
    "    noise_8_sigma = noise_sigma * 8.\n",
    "\n",
    "    #print(noise_mean, noise_3_sigma, noise_8_sigma)\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.imshow(noise_cutout.data, vmax=noise_mean+noise_3_sigma, vmin=noise_mean-noise_3_sigma)\n",
    "        plt.show()\n",
    "    \n",
    "        n, bins, patches = plt.hist(noise_cutout.data.flatten(), bins=35, align='left', color='black')\n",
    "        plt.plot(bins[:-1], n, c='r', linewidth=3)\n",
    "\n",
    "        plt.xlabel('Flux Bins [{}]'.format(str(data.unit)))\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Noise Histogram')\n",
    "        plt.show()\n",
    "    \n",
    "    # Cutout Image\n",
    "    # -------------\n",
    "    image = Cutout2D(data, (cx, cy), data_cutout_size, copy=True)\n",
    "    rms_image = Cutout2D(rms_data, (cx, cy), data_cutout_size, copy=True)\n",
    "\n",
    "    # Compute image stats\n",
    "    image_min = image.data.min()\n",
    "    image_max = image.data.max()\n",
    "    image_mean = image.data.mean()\n",
    "    image_sigma = image.data.std()\n",
    "    image_3_sigma = image_sigma * 3.\n",
    "    image_8_sigma = image_sigma * 8.\n",
    "\n",
    "    image_clipped_mean, image_clipped_median, image_clipped_std = sigma_clipped_stats(image.data, sigma=3.0)\n",
    "\n",
    "    # Subtract Noise\n",
    "    #image.data  -= noise_mean\n",
    "\n",
    "    # Set min and max values for all plots\n",
    "    vmin = -10 * image_clipped_std\n",
    "    vmax = +10 * image_clipped_std\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.imshow(image.data, vmin=vmin, vmax=vmax)\n",
    "        plt.show()\n",
    "\n",
    "    #print(image_mean)\n",
    "    \n",
    "    # Segmentation\n",
    "    # ------------\n",
    "    \n",
    "    # Make segmentation map\n",
    "    segm = detection_cat\n",
    "\n",
    "    # Deblend segmentation map\n",
    "    segm_deblend = detection_segm_deblend\n",
    "\n",
    "    # Remove border sources\n",
    "    #segm_deblend.remove_border_labels(1)\n",
    "\n",
    "    # Make catalog\n",
    "    cat = detection_cat\n",
    "\n",
    "    # Display source properties\n",
    "    #print(\"Num of Targets:\", len(cat))\n",
    "\n",
    "    # Plot segments\n",
    "    if show_plots:\n",
    "        plot_segments(segm_deblend, image.data, vmin=vmin, vmax=vmax)\n",
    "        plt.show()\n",
    "        \n",
    "    # Photomerty\n",
    "    # ----------\n",
    "    pb = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(cat),\n",
    "        step=1,\n",
    "        description='Loading:',\n",
    "        bar_style='',\n",
    "        orientation='horizontal',\n",
    "        style=DISPLAY_STYLE,\n",
    "        \n",
    "    )\n",
    "    display(pb)\n",
    "\n",
    "    photometry_rows = []\n",
    "    error_rows = []\n",
    "    mag_rows = []\n",
    "    mag_error_rows = []\n",
    "    for idx, obj in enumerate(cat):\n",
    "        pb.value = idx + 1\n",
    "        pb.description = \"{}: {}/{}\".format(current_filter, pb.value, len(cat))\n",
    "        \n",
    "        petrosian_properties = petrosian_properties_rows[idx]\n",
    "        \n",
    "        r_total_flux = petrosian_properties.r_total_flux\n",
    "        r_half_light = petrosian_properties.r_half_light\n",
    "        if np.any(np.isnan(np.array([r_half_light, r_total_flux]))):\n",
    "            nan_values = np.array([np.nan, np.nan])\n",
    "            photometry_rows.append(nan_values)\n",
    "            error_rows.append(nan_values)\n",
    "            mag_rows.append(nan_values)\n",
    "            mag_error_rows.append(nan_values)\n",
    "            continue\n",
    "            \n",
    "        bbox = max(obj.bbox.ixmax - obj.bbox.ixmin, obj.bbox.iymax - obj.bbox.iymin)\n",
    "        cutout_size = max(obj.bbox.ixmax - obj.bbox.ixmin, obj.bbox.iymax - obj.bbox.iymin)#r_total_flux * 2\n",
    "        cutout_size = r_total_flux * 2.5\n",
    "         \n",
    "        flux_arr, area_arr, error_arr = object_photometry(\n",
    "            obj, image.data, \n",
    "            segm_deblend, [r_half_light, r_total_flux],\n",
    "            error=rms_image.data,\n",
    "            plot=show_plots, sigma=1, vmax=vmax, vmin=vmin, \n",
    "            bkg_sub=True, mask_background=False, \n",
    "            cutout_size=cutout_size\n",
    "        )\n",
    "        \n",
    "        #error_arr = np.sqrt(flux_arr)\n",
    "        \n",
    "        photometry_rows.append(flux_arr)\n",
    "        error_rows.append(error_arr)\n",
    "        mag_rows.append(flux_to_abmag(flux_arr, data.header))\n",
    "        mag_error_rows.append(flux_to_abmag(flux_arr, data.header))\n",
    "\n",
    "\n",
    "    return photometry_rows, error_rows, mag_rows, mag_error_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing \n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=cpu_count()) as pool: \n",
    "        print(\"Starting MP\")\n",
    "        try:\n",
    "            photometry_results = pool.map(photometry_process_image, acs_filter_list + wfc3_filter_list)\n",
    "        except Exception as e:\n",
    "            pool.close()\n",
    "            raise e \n",
    "            \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-tribune",
   "metadata": {},
   "source": [
    "# Combine Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "column_names = []\n",
    "\n",
    "r_petrosian_list = []\n",
    "r_half_light_list = []\n",
    "r_half_light_arcsec_list = []\n",
    "r_total_flux_list = []\n",
    "r_total_flux_arcsec_list = []\n",
    "r20_list = []\n",
    "r80_list = []\n",
    "c2080_list = []\n",
    "\n",
    "for p in petrosian_properties_rows:\n",
    "    r_petrosian_list.append(p.r_petrosian)\n",
    "    r_half_light_list.append(p.r_half_light)\n",
    "    r_half_light_arcsec_list.append(p.r_half_light_arcsec)\n",
    "    r_total_flux_list.append(p.r_total_flux)\n",
    "    r_total_flux_arcsec_list.append(p.r_total_flux_arcsec)\n",
    "    r20_list.append(p.concentration_index()[0])\n",
    "    r80_list.append(p.concentration_index()[1])\n",
    "    c2080_list .append(p.concentration_index()[2])    \n",
    "    \n",
    "columns.append(np.array(r_petrosian_list))\n",
    "column_names.append('r_petrosian')\n",
    "\n",
    "columns.append(np.array(r_half_light_list))\n",
    "column_names.append('r_half_light')\n",
    "\n",
    "columns.append(np.array(r_half_light_arcsec_list))\n",
    "column_names.append('r_half_light_arcsec')\n",
    "\n",
    "columns.append(np.array(r_total_flux_list))\n",
    "column_names.append('r_total_flux')\n",
    "\n",
    "columns.append(np.array(r_total_flux_arcsec_list))\n",
    "column_names.append('r_total_flux_arcsec')\n",
    "\n",
    "columns.append(np.array(r20_list))\n",
    "column_names.append('r20')\n",
    "\n",
    "columns.append(np.array(r80_list))\n",
    "column_names.append('r80')\n",
    "\n",
    "columns.append(np.array(c2080_list))\n",
    "column_names.append('c2080')\n",
    "\n",
    "for idx,  filt in  enumerate(acs_filter_list + wfc3_filter_list):\n",
    "    print(filt)\n",
    "    f1, f2 = np.transpose(np.array(photometry_results[idx][0]))\n",
    "    err1, err2 = np.transpose(np.array(photometry_results[idx][1]))\n",
    "    m1, m2 = np.transpose(np.array(photometry_results[idx][2]))\n",
    "    merr1, merr2 = np.transpose(np.array(photometry_results[idx][3]))\n",
    "    \n",
    "    # Plot \n",
    "    total_hl_diff = m2 - m1 + 2.5*np.log10(2)\n",
    "    \n",
    "    plt.scatter(m2, total_hl_diff, label=filt)\n",
    "    plt.title(' m_total - \"double\" m_half_light')\n",
    "    plt.xlabel(\"m_total\")\n",
    "    plt.ylabel(\"m_total - m_half_light + 2.5 * log_10(2)\")\n",
    "    \n",
    "    columns.append(f1)\n",
    "    column_names.append('f_half_light_{}'.format(filt))\n",
    "    \n",
    "    columns.append(err1)\n",
    "    column_names.append('f_half_light_err_{}'.format(filt))\n",
    "    \n",
    "    columns.append(f2)\n",
    "    column_names.append('f_total_{}'.format(filt))\n",
    "\n",
    "    columns.append(err2)\n",
    "    column_names.append('f_total_err_{}'.format(filt))\n",
    "    \n",
    "    columns.append(m1)\n",
    "    column_names.append('m_half_light_{}'.format(filt))\n",
    "    \n",
    "    columns.append(merr1)\n",
    "    column_names.append('m_half_light_err_{}'.format(filt))\n",
    "    \n",
    "    columns.append(m2)\n",
    "    column_names.append('m_total_{}'.format(filt))\n",
    "    \n",
    "    columns.append(merr2)\n",
    "    column_names.append('m_total_err_{}'.format(filt))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "petrosian_cat = detection_cat.to_table().copy()\n",
    "\n",
    "ra, dec = detection_image.wcs.all_pix2world(petrosian_cat['maxval_xpos'], petrosian_cat['maxval_ypos'], 0)\n",
    "\n",
    "petrosian_cat.add_column(ra*u.deg, index=1, name='ra')\n",
    "petrosian_cat.add_column(dec*u.deg, index=2, name='dec')\n",
    "\n",
    "for c, cn in zip(columns, column_names):\n",
    "    petrosian_cat.add_column(c, name=cn)\n",
    "petrosian_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_catalog_output_path = \"{}_petrosian_catalog.csv\".format(program_name)\n",
    "print(final_catalog_output_path)\n",
    "\n",
    "petrosian_cat.write(final_catalog_output_path, overwrite=True)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-miller",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
